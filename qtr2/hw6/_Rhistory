adult.data.cat$`hours-per-week` <- cut(adult.data$'hours-per-week', c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
head(adult.data.cat)
adult.data.cat$`hours-per-week` <- cut(adult.data$`hours-per-week`, c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
head(adult.data.cat)
mydata = read.csv("C:\Users\kevin\Downloads\CollegeScorecard_Raw_Data\CollegeScorecard_Raw_Data\MERGED2013_PP.csv")
mydata = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2013_PP.csv")
head(mydata)
summary(mydata)
View(mydata)
View(mydata)
View(mydata)
View(mydata)
mydata = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2013_PP.csv")
View(mydata)
View(mydata)
class(mydata)
table(mydata$region)
table(mydata$region, mydata$PREDDEG)
table(mydata$PREDDEG)
head(mydata$program_percentage.construction)
head(mydata$CIP13BACHL)
head(mydata$CIP13BACHL, n=10)
head(mydata$CIP13BACHL, n=40)
head(mydata$CIP04BACHL, n=40)
head(mydata$OVERALL_YR6_N, n=40)
head(mydata$OVERALL_YR8_N, n=40)
head(mydata$OVERALL_YR2_N, n=40)
head(mydata$OVERALL_YR1_N, n=40)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2016_PP.csv")
table(mydata$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006PP.csv")
table(mydata$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006PP.csv")
table(mydata2006$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
table(mydata2006$OVERALL_YR6_N)
head(table(mydata2006$OVERALL_YR6_N))
head(mydata2006$OVERALL_YR6_N)
sum(mydata2006$OVERALL_YR6_N)
sum(mydata2006$OVERALL_YR6_N[OVERALL_YR6_N!='NULL'])
sum(mydata2006$OVERALL_YR6_N[$OVERALL_YR6_N!='NULL'])
sum(mydata2006$OVERALL_YR6_N[mydata2006$OVERALL_YR6_N!='NULL'])
sum(as.numeric(mydata2006$OVERALL_YR6_N))
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
mean(mydata2006$PCIP14)
mean(as.numeric(mydata2006$PCIP14))
colMeans(mydata2006, na.rm = TRUE)
colMeans(as.numeric(mydata2006), na.rm = TRUE)
colMeans(mydata2006, na.rm = TRUE)
mean(as.numeric(mydata2006$PCIP14), na.rm=TRUE)
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
View(degSalary)
View(degSalary)
View(mydata2006)
View(degSalary)
View(degSalary)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
View(degSalary)
View(degSalary)
meanEarn <- mydata2006[, 1691]
meanEarn[meanEarn=="NULL"] <- NA
meanEarn[meanEarn=="PrivacySuppressed"] <- NA
meanEarn <- sapply(meanEarn, function(x) as.numeric(as.character(x)) )
meanEarnDF <- data.frame(meanEarn)
meanEarnDF
meanEarn <- mydata2006[, 1691]
meanEarn[meanEarn=="NULL"] <- NA
meanEarn[meanEarn=="PrivacySuppressed"] <- NA
meanEarn <- sapply(meanEarn, function(x) as.numeric(as.character(x)) )
meanEarnDF <- data.frame(meanEarn)
meanEarnDF
value <- mydata2006[c("INSTNM", "mn_earn_wne_p7","TUITIONFEE_IN")]
value[value=="NULL"] <- NA
value[value=="PrivacySuppressed"] <- NA
value <- na.omit(value)
value[-1] <- sapply(value[-1], function(x) as.numeric(as.character(x)) )
value$rank <- value$TUITIONFEE_IN/value$mn_earn_wne_p7
value <- value[order(value$rank),]
head(value)
?sample
?help
#-----------------------------
#
#  Weather Scraper Function
#
#  Purpose: Get and store data from wunderground
#
#  Created by: Nikola Tesla (ntesla@uw.edu)
#
#  Created on: 2015-05-07
#
#-----------------------------
##----Import Libraries-----
require(RSQLite)
require(logging)
##----Define the get-weather-data function-----
get_weather_data = function(airport, dates, logger=NA, db_conn=NA){
# Build HTML Link String
site_prefix = 'http://www.wunderground.com/history/airport/'
site_suffix = '/DailyHistory.html?format=1'
weather_links = paste0(site_prefix,airport,'/',gsub('-','/',dates),site_suffix)
# Initialize final data frame
weather_frame = data.frame()
# Get Data
for (l in weather_links){
print(paste('Getting link',l))
# Log each attempt
if (is.function(logger)){
loginfo(paste('Getting link',l),logger=logger)
}
weather_info = tryCatch(readLines(l)[-1], # Get String Response
error = function(e){
print(paste('Error getting',l)) # Output Error on Screen/Console
if(is.function(logger)){loginfo(paste('Error getting',l)
,logger=logger)} # Store Error in Log
})
weather_info = strsplit(weather_info,',') # Parse each line by  a comma
headers = weather_info[[1]]               # Get Headers
weather_info = weather_info[-1]           # Drop Headers
# Now transform list into data frame
weather_info = do.call(rbind.data.frame, weather_info)
names(weather_info) = headers
# Post Retrieval Data Cleanup
weather_info <- data.frame(lapply(weather_info, as.character),
stringsAsFactors=FALSE)
# Convert numeric columns to numbers
numeric_cols = c(2,3,4,5,6,8,9,10,13)
weather_info[numeric_cols] = lapply(weather_info[numeric_cols],as.numeric)
# Fill in the 'dashes' to zero
weather_info[is.na(weather_info)]=0
# Rename the date column and drop the last html tag
colnames(weather_info)[14]="Date"
weather_info$Date = as.Date(substr(weather_info$Date,1,10))
# Concatenate DFs together
weather_frame = tryCatch(rbind(weather_frame, setNames(weather_info, names(weather_frame))),
error=function(e) {print(e);weather_frame})
} # End loop through each day's weather csv link (l)
# Log ending time
if(is.function(logger)){
loginfo('All done!',logger=logger)
}
# Write to SQLite DB
if(isS4(db_conn)){
dbWriteTable(db_conn, airport, weather_frame, overwrite=TRUE)
}
names(weather_frame) = c('time','temp','dew_pt','humidity','pressure',
'visibility','wind_dir','wind_speed','gust_speed',
'precipitation','events','conditions',
'wind_dir_deg','date')
return(weather_frame)
}
if(interactive()){
##----Setup Test Logger-----
basicConfig()
addHandler(writeToFile, file="~/testing.log", level='DEBUG')
##----Test Parameters----
airport = 'KSEA'
dates = seq(from=as.Date('2015-05-01'),
to=as.Date('2015-05-06'),
by=1)
sql_db_name = 'weather.db'
##----Connect to SQLite DB----
con = dbConnect(SQLite(), dbname=sql_db_name)
weather_data = get_weather_data(airport, dates)
dbDisconnect(con)
}
install(RSQLite)
install(logging)
require(RSQLite)
require(logging)
install.packages("twitteR")
install.packages("devtools")
library(twitteR)
library(devtools)
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
library(httr)
setwd('C:\Users\kevin\Google Drive\datasci\hw2')
twit_cred = read.csv('twitter_cred.csv', stringsAsFactors=FALSE)
TWITTER_CONSUMER_KEY = twit_cred$TWITTER_CONSUMER_KEY
TWITTER_CONSUMER_SECRET = twit_cred$TWITTER_CONSUMER_SECRET
TWITTER_ACCESS_TOKEN = twit_cred$TWITTER_ACCESS_TOKEN
TWITTER_ACCESS_SECRET = twit_cred$TWITTER_ACCESS_SECRET
setup_twitter_oauth(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET,
TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)
ds <- searchTwitter('#datascience', locale=NULL, geocode=NULL,n=500)
library(twitteR)
library(devtools)
library(httr)
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
library(twitteR)
library(devtools)
?devtools::install_version
library(httr)
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
library(twitteR)
library(devtools)
library(twitteR)
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
library(httr)
library(twitteR)
library(devtools)
clear
cls
library(twitteR)
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
install.packages("digest")
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
devtools::install_version("httr", version="0.6.0", repos="http://cran.us.r-project.org")
require(logging)
r -version
help()
version
mydata = read.csv("C:\temp\farm_data.txt")
str(mydata)
mydata = read.csv("C:\temp\farm_data.txt")
mydata <= read.csv("C:\temp\farm_data.txt")
mydata <= read.csv("C:\temp\farm_data.txt")
mydata <- read.csv("C:\temp\farm_data.txt")
mydata <- read.csv("C:/temp/farm_data.txt")
str(mydata)
View(mydata)
View(mydata)
?read.csv
mydata <- read.csv("C:/temp/farm_data.txt",stringsAsFactors=FALSE, sep=";")
str(mydata)
View(mydata)
View(mydata)
#HW 6 Kevin Niemann
setwd('C:/Users/kevin/Google Drive/datasci/HW6')
#install.packages("pls")
#install.packages("glmnet")
library(pls)
library(glmnet)
set.seed(20) # You will need to set the seed to 20 for this to work.
# Retrieve Breast Cancer Expression Data From the following Study:
#http://www.ncbi.nlm.nih.gov/pubmed/21532620
micro_data=read.table("MicroArray.txt", header=TRUE)
dim(micro_data)
# Normalize each column
micro_data = scale(micro_data)
# Breast Cancer Samples:
cancer_samples = c(0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,1)
# Think of this as ~ 10,000 possible variables (genes) to predict 19 outcomes.
# Convert to data.frame
micro_frame = data.frame(t(micro_data))
micro_frame$outcomes = cancer_samples
##-----Lasso Regression-----
# user glmnet, where family = 'binomial'
inputs = model.matrix(outcomes ~ ., data = micro_frame)
cancer_lasso = glmnet(inputs, micro_frame$outcomes, family='binomial', alpha = 1)
# See how the variables vary with the different regularization factor, lambda
coef(cancer_lasso)[,20][coef(cancer_lasso)[,20]>1e-10]
plot(cancer_lasso, xvar="lambda")
# Now use cv.glmnet to test different lasso cutoffs
cancer_lasso_cv = cv.glmnet(inputs,micro_frame$outcomes,alpha=1,family='binomial')
plot(cancer_lasso_cv)
# find the minumum lambda.min
best_lambda = cancer_lasso_cv$lambda.min
# Find the coefficients that are greater than zero
best_coef = coef(cancer_lasso)[,cancer_lasso$lambda == best_lambda]
best_coef = best_coef[best_coef > 1e-10]
# Plug this into the glm(...,family='binomial') to get the logistic outcome
inputFormula = paste(names(best_coef[-1]), collapse =" + ")
formula = as.formula(paste('outcomes ~', inputFormula, sep=""))
has_cancer = glm(formula,family='binomial', data=micro_frame)
# Compare with the real outcome, cancer_samples above
summary(has_cancer)
evacuated = c(0,0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0)
pets = c(1,1,1,1,0,0,0,1,1,0,0,1,1,1,1,1,1,0,0)
mobile_home = c(0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0)
tenure = c(16,26,11,1,5,34,3,3,10,2,2,25,20,11,15,21,3,5,7)
years_educ = c(16,12,13,10,12,12,14,16,12,18,12,16,12,10,8,15,22,18,18)
evac_df = data.frame("evacuated" = evacuated,
"pets" = pets,
"mobile_home" = mobile_home,
"tenure" = tenure,
"years_educ" = years_educ)
evac_lm = lm(evacuated ~ pets + mobile_home + tenure + years_educ, data = evac_df)
summary(evac_lm)
# What's the problem here?
range(evac_lm$fitted.values)
evac_logit = glm(evacuated ~ pets + mobile_home + tenure + years_educ, data = evac_df, family = binomial)
# Remember that if each error is  bernoulli distributed, then
#  the distribution of many errors is binomial (multiple trials).
summary(evac_logit)
cutoff = 0.5
prediction = as.numeric(evac_logit$fitted.values>cutoff)
prediction
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
prediction
cancer_samples
?formula
inputs = model.matrix(outcomes ~ . - outcomes, data = micro_frame)
inputs
#HW 6 Kevin Niemann
setwd('C:/Users/kevin/Google Drive/datasci/HW6')
#install.packages("pls")
#install.packages("glmnet")
library(pls)
library(glmnet)
set.seed(20) # You will need to set the seed to 20 for this to work.
# Retrieve Breast Cancer Expression Data From the following Study:
#http://www.ncbi.nlm.nih.gov/pubmed/21532620
micro_data=read.table("MicroArray.txt", header=TRUE)
dim(micro_data)
# Normalize each column
micro_data = scale(micro_data)
# Breast Cancer Samples:
cancer_samples = c(0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,1)
# Think of this as ~ 10,000 possible variables (genes) to predict 19 outcomes.
# Convert to data.frame
micro_frame = data.frame(t(micro_data))
micro_frame$outcomes = cancer_samples
##-----Lasso Regression-----
# user glmnet, where family = 'binomial'
inputs = model.matrix(outcomes ~ . - outcomes, data = micro_frame)
cancer_lasso = glmnet(inputs, micro_frame$outcomes, family='binomial', alpha = 1)
# See how the variables vary with the different regularization factor, lambda
coef(cancer_lasso)[,20][coef(cancer_lasso)[,20]>1e-10]
plot(cancer_lasso, xvar="lambda")
# Now use cv.glmnet to test different lasso cutoffs
cancer_lasso_cv = cv.glmnet(inputs,micro_frame$outcomes,alpha=1,family='binomial')
plot(cancer_lasso_cv)
# find the minumum lambda.min
best_lambda = cancer_lasso_cv$lambda.min
# Find the coefficients that are greater than zero
best_coef = coef(cancer_lasso)[,cancer_lasso$lambda == best_lambda]
best_coef = best_coef[best_coef > 1e-10]
# Plug this into the glm(...,family='binomial') to get the logistic outcome
inputFormula = paste(names(best_coef[-1]), collapse =" + ")
formula = as.formula(paste('outcomes ~', inputFormula, sep=""))
has_cancer = glm(formula,family='binomial', data=micro_frame)
summary(has_cancer)
# Compare with the real outcome, cancer_samples above
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
prediction
cancer_samples
inputFormula
formula
micro_frame$outcomes = NULL
has_cancer = glm(formula,family='binomial', data=micro_frame)
summary(has_cancer)
# Compare with the real outcome, cancer_samples above
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
prediction
cancer_samples
micro_frame$outcomes = cancer_samples
prediction = as.numeric(has_cancer$fitted.values>cutoff)
pred_cancer_actual_cancer = sum( (prediction == 1) & (cancer_samples == 1) )
pred_no_cancer_actual_no_cancer = sum( (prediction == 0) & (cancer_samples == 0) )
accuracy = (pred_cancer_actual_cancer + pred_no_cancer_actual_no_cancer)/(nrow(micro_frame))
accuracy
nrow(micro_frame)
ncol(micro_frame)
coef(cancer_lasso)[,20][coef(cancer_lasso)[,20]>1e-10]
plot(cancer_lasso, xvar="lambda")
cancer_lasso_cv = cv.glmnet(inputs,micro_frame$outcomes,alpha=1,family='binomial')
plot(cancer_lasso_cv)
?"glmnet"
?cv.glmnet
has_cancer$fitted.values
prediction = as.numeric(has_cancer$fitted.values>cutoff)
prediction
pred_cancer_actual_cancer = sum( (prediction == 1) & (cancer_samples == 1) )
pred_no_cancer_actual_no_cancer
pred_cancer_actual_cancer
pred_no_cancer_actual_no_cancer
?"glmnet"
?cv.glmnet
?glm
micro_frame$outcomes = NULL
has_cancer = glm(formula,family='binomial', data=micro_frame)
summary(has_cancer)
# Compare with the real outcome, cancer_samples above
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
pred_cancer_actual_cancer = sum( (prediction == 1) & (cancer_samples == 1) )
pred_no_cancer_actual_no_cancer = sum( (prediction == 0) & (cancer_samples == 0) )
# Maybe say our accuracy is total right out of total:
accuracy = (pred_cancer_actual_cancer + pred_no_cancer_actual_no_cancer)/(nrow(micro_frame))
accuracy
#HW 6 Kevin Niemann
setwd('C:/Users/kevin/Google Drive/datasci/HW6')
#install.packages("pls")
#install.packages("glmnet")
library(pls)
library(glmnet)
set.seed(20) # You will need to set the seed to 20 for this to work.
# Retrieve Breast Cancer Expression Data From the following Study:
#http://www.ncbi.nlm.nih.gov/pubmed/21532620
micro_data=read.table("MicroArray.txt", header=TRUE)
dim(micro_data)
# Normalize each column
micro_data = scale(micro_data)
# Breast Cancer Samples:
cancer_samples = c(0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,1)
# Think of this as ~ 10,000 possible variables (genes) to predict 19 outcomes.
# Convert to data.frame
micro_frame = data.frame(t(micro_data))
micro_frame$outcomes = cancer_samples
##-----Lasso Regression-----
# user glmnet, where family = 'binomial'
inputs = model.matrix(outcomes ~ . - outcomes, data = micro_frame)
cancer_lasso = glmnet(inputs, micro_frame$outcomes, family='binomial', alpha = 1)
# See how the variables vary with the different regularization factor, lambda
coef(cancer_lasso)[,20][coef(cancer_lasso)[,20]>1e-10]
plot(cancer_lasso, xvar="lambda")
# Now use cv.glmnet to test different lasso cutoffs
cancer_lasso_cv = cv.glmnet(inputs,micro_frame$outcomes,alpha=1,family='binomial')
plot(cancer_lasso_cv)
# find the minumum lambda.min
best_lambda = cancer_lasso_cv$lambda.min
# Find the coefficients that are greater than zero
best_coef = coef(cancer_lasso)[,cancer_lasso$lambda == best_lambda]
best_coef = best_coef[best_coef > 1e-10]
# Plug this into the glm(...,family='binomial') to get the logistic outcome
inputFormula = paste(names(best_coef[-1]), collapse =" + ")
formula = as.formula(paste('outcomes ~', inputFormula, sep=""))
micro_frame$outcomes = NULL
has_cancer = glm(formula,family='binomial', data=micro_frame)
summary(has_cancer)
# Compare with the real outcome, cancer_samples above
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
pred_cancer_actual_cancer = sum( (prediction == 1) & (cancer_samples == 1) )
pred_no_cancer_actual_no_cancer = sum( (prediction == 0) & (cancer_samples == 0) )
# Maybe say our accuracy is total right out of total:
accuracy = (pred_cancer_actual_cancer + pred_no_cancer_actual_no_cancer)/(nrow(micro_frame))
#Summary
#The breat cancer data was fit to a generalized linear model through a lasso method through the glmnet function.
#I then used cross validation for the glmnet function through cv.glmnet and determined the minimum lambda value.
#With that value to find the best coefficents produced by the glm function.
#When I plugged those coefficients into the right side of the formula of the glm function, I tested the new model to
#predict which patients had cancer. It correctly predicted the 5 cancer cases and 14 non-cancer cases.
accuracy
#HW 6 Kevin Niemann
setwd('C:/Users/kevin/Google Drive/datasci/HW6')
#install.packages("pls")
#install.packages("glmnet")
library(pls)
library(glmnet)
set.seed(20) # You will need to set the seed to 20 for this to work.
# Retrieve Breast Cancer Expression Data From the following Study:
#http://www.ncbi.nlm.nih.gov/pubmed/21532620
micro_data=read.table("MicroArray.txt", header=TRUE)
dim(micro_data)
# Normalize each column
micro_data = scale(micro_data)
# Breast Cancer Samples:
cancer_samples = c(0,0,0,0,0,1,0,0,0,1,0,1,0,0,1,0,0,0,1)
# Think of this as ~ 10,000 possible variables (genes) to predict 19 outcomes.
# Convert to data.frame
micro_frame = data.frame(t(micro_data))
micro_frame$outcomes = cancer_samples
##-----Lasso Regression-----
# user glmnet, where family = 'binomial'
inputs = model.matrix(outcomes ~ . - outcomes, data = micro_frame)
cancer_lasso = glmnet(inputs, micro_frame$outcomes, family='binomial', alpha = 1)
# See how the variables vary with the different regularization factor, lambda
coef(cancer_lasso)[,20][coef(cancer_lasso)[,20]>1e-10]
plot(cancer_lasso, xvar="lambda")
# Now use cv.glmnet to test different lasso cutoffs
cancer_lasso_cv = cv.glmnet(inputs,micro_frame$outcomes,alpha=1,family='binomial')
plot(cancer_lasso_cv)
# find the minumum lambda.min
best_lambda = cancer_lasso_cv$lambda.min
# Find the coefficients that are greater than zero
best_coef = coef(cancer_lasso)[,cancer_lasso$lambda == best_lambda]
best_coef = best_coef[best_coef > 1e-10]
# Plug this into the glm(...,family='binomial') to get the logistic outcome
inputFormula = paste(names(best_coef[-1]), collapse =" + ")
formula = as.formula(paste('outcomes ~', inputFormula, sep=""))
has_cancer = glm(formula,family='binomial', data=micro_frame)
summary(has_cancer)
# Compare with the real outcome, cancer_samples above
cutoff = 0.5
prediction = as.numeric(has_cancer$fitted.values>cutoff)
pred_cancer_actual_cancer = sum( (prediction == 1) & (cancer_samples == 1) )
pred_no_cancer_actual_no_cancer = sum( (prediction == 0) & (cancer_samples == 0) )
# Maybe say our accuracy is total right out of total:
accuracy = (pred_cancer_actual_cancer + pred_no_cancer_actual_no_cancer)/(nrow(micro_frame))
#Summary
#The breat cancer data was fit to a generalized linear model through a lasso method through the glmnet function.
#I then used cross validation for the glmnet function through cv.glmnet and determined the minimum lambda value.
#With that min/best lambda, I found the best coefficents produced by the glm function.
#When I plugged those coefficients into the right side of the formula of the glm function, I tested the new model to
#predict which patients had cancer. It correctly predicted the 5 cancer cases and 14 non-cancer cases.
