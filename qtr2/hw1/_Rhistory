plot(traintest)
traintest=rbind(train, test)
plot(traintest, xlab=X, ylab=Y)
traintest=rbind(train, test)
plot(traintest, xlab="X", ylab="Y")
cl=factor(c(rep("A",3),rep("B",3)))
cl
temp <- tempfile()
download.file("http://www.grouplens.org/system/files/ml-100k.zip",temp)
data <- read.table(unz(temp, "a1.dat"))
unlink(temp)
icecream <- data.frame(
temp=c(11.9, 14.2, 15.2, 16.4, 17.2, 18.1,
18.5, 19.4, 22.1, 22.6, 23.4, 25.1),
units=c(185L, 215L, 332L, 325L, 408L, 421L,
406L, 412L, 522L, 445L, 544L, 614L)
)
````
**b**\. Create a plot units of ice cream sold by temperature
````{r echo=TRUE}
plot(icecream)
plot(icecream)
install.packages("arules")
install.packages("arulesViz")
install.packages("DMwR")
#
# Text Mining
install.packages("tm")
install.packages("wordcloud")
install.packages("SnowballC")
install.packages("zoo")
#
install.packages("twitteR")
install.packages("ROAuth")
install.packages("RCurl")
library(ggplot2)
library(dplyr)
library(arules)
library(arulesViz)
library(RColorBrewer)
library(DMwR)
library(tm)
library(SnowballC)
library(wordcloud)
library(zoo)
library(twitteR)
library(ROAuth)
library(RCurl)
adult.data <- getURL(https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data)
adult.data <- getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"")
data
data()
exit
bye
quit
help
adult.data <- getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data")
adult.data
tmp <- getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data")
adult.data <- read.csv(text = tmp)
tmp2 <- getURL("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names")
adult.names <- read.csv(text = tmp2)
View(adult.data)
View(adult.data)
test <- read.csv(
"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
header=FALSE)
test <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",header=FALSE)
names(test) <- c("A","B","C","D","E","F","G","H","I","J","K")
View(test)
View(test)
View(adult.data)
View(adult.data)
adult.data <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",header=FALSE)
names(adult.data) <- c("A","B","C","D","E","F","G","H","I","J","K")
View(adult.data)
View(adult.data)
head(adult.data)
names(adult.data) <- c("Age","Workclass","fnlwgt","education","education-num","martial-status","occupation","inspct","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country")
head(adult.data)
head(adult.data)
names(adult.data) <- c("Age","Workclass","fnlwgt","education","education-num","martial-status","occupation","relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country")
head(adult.data)
tail(adult.data)
cut(adult.data$age, c(0, 15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data$Age
cut(adult.data$Age, c(0, 15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
cut(adult.data$age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
head(adult.data)
cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat <- cut(adult.data$age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat <- cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
head(adult.data.cat)
cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat <- adult.data
adult.data.cat$Age <- cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
head(adult.data.cat)
adult.data.cat <- adult.data
adult.data.cat$Age <- cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat$hours-per-week <- cut(adult.data$hours-per-week, c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
adult.data.cat <- adult.data
adult.data.cat$Age <- cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat$`hours-per-week` <- cut(adult.data$hours-per-week, c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
adult.data.cat <- adult.data
adult.data.cat$Age <- cut(adult.data$Age, c(15, 25, 45,65,100), labels=c('Young', 'Middle-aged', 'Senior', 'Elder'))
adult.data.cat$`hours-per-week` <- cut(adult.data$'hours-per-week', c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
head(adult.data.cat)
adult.data.cat$`hours-per-week` <- cut(adult.data$`hours-per-week`, c(0, 25, 40,60,168), labels=c('Part-time', 'Full-time', 'Over-time', 'Workaholic'))
head(adult.data.cat)
mydata = read.csv("C:\Users\kevin\Downloads\CollegeScorecard_Raw_Data\CollegeScorecard_Raw_Data\MERGED2013_PP.csv")
mydata = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2013_PP.csv")
head(mydata)
summary(mydata)
View(mydata)
View(mydata)
View(mydata)
View(mydata)
mydata = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2013_PP.csv")
View(mydata)
View(mydata)
class(mydata)
table(mydata$region)
table(mydata$region, mydata$PREDDEG)
table(mydata$PREDDEG)
head(mydata$program_percentage.construction)
head(mydata$CIP13BACHL)
head(mydata$CIP13BACHL, n=10)
head(mydata$CIP13BACHL, n=40)
head(mydata$CIP04BACHL, n=40)
head(mydata$OVERALL_YR6_N, n=40)
head(mydata$OVERALL_YR8_N, n=40)
head(mydata$OVERALL_YR2_N, n=40)
head(mydata$OVERALL_YR1_N, n=40)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2016_PP.csv")
table(mydata$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006PP.csv")
table(mydata$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006PP.csv")
table(mydata2006$OVERALL_YR6_N)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
table(mydata2006$OVERALL_YR6_N)
head(table(mydata2006$OVERALL_YR6_N))
head(mydata2006$OVERALL_YR6_N)
sum(mydata2006$OVERALL_YR6_N)
sum(mydata2006$OVERALL_YR6_N[OVERALL_YR6_N!='NULL'])
sum(mydata2006$OVERALL_YR6_N[$OVERALL_YR6_N!='NULL'])
sum(mydata2006$OVERALL_YR6_N[mydata2006$OVERALL_YR6_N!='NULL'])
sum(as.numeric(mydata2006$OVERALL_YR6_N))
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
head(mydata2006$OVERALL_YR6_N)
mean(mydata2006$PCIP14)
mean(as.numeric(mydata2006$PCIP14))
colMeans(mydata2006, na.rm = TRUE)
colMeans(as.numeric(mydata2006), na.rm = TRUE)
colMeans(mydata2006, na.rm = TRUE)
mean(as.numeric(mydata2006$PCIP14), na.rm=TRUE)
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
View(degSalary)
View(degSalary)
View(mydata2006)
View(degSalary)
View(degSalary)
mydata2006 = read.csv("C:/Users/kevin/Downloads/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/MERGED2006_PP.csv")
degSalary <- mydata2006[, 62:99]
degSalary[degSalary=="NULL"] <- NA
degSalary[, 1:38] <- sapply(degSalary[, 1:38], function(x) as.numeric(as.character(x)) )
avg = colMeans(degSalary, na.rm=TRUE)
avgDF <- data.frame(as.list(avg))
degSalary$mn_earn_wne_p7 <- mydata2006[c("mn_earn_wne_p7","TUITIONFEE_IN")]
View(degSalary)
View(degSalary)
meanEarn <- mydata2006[, 1691]
meanEarn[meanEarn=="NULL"] <- NA
meanEarn[meanEarn=="PrivacySuppressed"] <- NA
meanEarn <- sapply(meanEarn, function(x) as.numeric(as.character(x)) )
meanEarnDF <- data.frame(meanEarn)
meanEarnDF
meanEarn <- mydata2006[, 1691]
meanEarn[meanEarn=="NULL"] <- NA
meanEarn[meanEarn=="PrivacySuppressed"] <- NA
meanEarn <- sapply(meanEarn, function(x) as.numeric(as.character(x)) )
meanEarnDF <- data.frame(meanEarn)
meanEarnDF
value <- mydata2006[c("INSTNM", "mn_earn_wne_p7","TUITIONFEE_IN")]
value[value=="NULL"] <- NA
value[value=="PrivacySuppressed"] <- NA
value <- na.omit(value)
value[-1] <- sapply(value[-1], function(x) as.numeric(as.character(x)) )
value$rank <- value$TUITIONFEE_IN/value$mn_earn_wne_p7
value <- value[order(value$rank),]
head(value)
?sample
?help
#-----------------------------
#
#  Weather Scraper Function
#
#  Purpose: Get and store data from wunderground
#
#  Created by: Nikola Tesla (ntesla@uw.edu)
#
#  Created on: 2015-05-07
#
#-----------------------------
##----Import Libraries-----
require(RSQLite)
require(logging)
##----Define the get-weather-data function-----
get_weather_data = function(airport, dates, logger=NA, db_conn=NA){
# Build HTML Link String
site_prefix = 'http://www.wunderground.com/history/airport/'
site_suffix = '/DailyHistory.html?format=1'
weather_links = paste0(site_prefix,airport,'/',gsub('-','/',dates),site_suffix)
# Initialize final data frame
weather_frame = data.frame()
# Get Data
for (l in weather_links){
print(paste('Getting link',l))
# Log each attempt
if (is.function(logger)){
loginfo(paste('Getting link',l),logger=logger)
}
weather_info = tryCatch(readLines(l)[-1], # Get String Response
error = function(e){
print(paste('Error getting',l)) # Output Error on Screen/Console
if(is.function(logger)){loginfo(paste('Error getting',l)
,logger=logger)} # Store Error in Log
})
weather_info = strsplit(weather_info,',') # Parse each line by  a comma
headers = weather_info[[1]]               # Get Headers
weather_info = weather_info[-1]           # Drop Headers
# Now transform list into data frame
weather_info = do.call(rbind.data.frame, weather_info)
names(weather_info) = headers
# Post Retrieval Data Cleanup
weather_info <- data.frame(lapply(weather_info, as.character),
stringsAsFactors=FALSE)
# Convert numeric columns to numbers
numeric_cols = c(2,3,4,5,6,8,9,10,13)
weather_info[numeric_cols] = lapply(weather_info[numeric_cols],as.numeric)
# Fill in the 'dashes' to zero
weather_info[is.na(weather_info)]=0
# Rename the date column and drop the last html tag
colnames(weather_info)[14]="Date"
weather_info$Date = as.Date(substr(weather_info$Date,1,10))
# Concatenate DFs together
weather_frame = tryCatch(rbind(weather_frame, setNames(weather_info, names(weather_frame))),
error=function(e) {print(e);weather_frame})
} # End loop through each day's weather csv link (l)
# Log ending time
if(is.function(logger)){
loginfo('All done!',logger=logger)
}
# Write to SQLite DB
if(isS4(db_conn)){
dbWriteTable(db_conn, airport, weather_frame, overwrite=TRUE)
}
names(weather_frame) = c('time','temp','dew_pt','humidity','pressure',
'visibility','wind_dir','wind_speed','gust_speed',
'precipitation','events','conditions',
'wind_dir_deg','date')
return(weather_frame)
}
if(interactive()){
##----Setup Test Logger-----
basicConfig()
addHandler(writeToFile, file="~/testing.log", level='DEBUG')
##----Test Parameters----
airport = 'KSEA'
dates = seq(from=as.Date('2015-05-01'),
to=as.Date('2015-05-06'),
by=1)
sql_db_name = 'weather.db'
##----Connect to SQLite DB----
con = dbConnect(SQLite(), dbname=sql_db_name)
weather_data = get_weather_data(airport, dates)
dbDisconnect(con)
}
install(RSQLite)
install(logging)
require(RSQLite)
require(logging)
plot(searchHC$Index,searchHC$HeadCount)
##--------------------------------------------
##
## R Review Homework Headstart
##
## Class: PCE Data Science Methods Class
##
##--------------------------------------------
##-----Set working directory-----
setwd('C:/Users/Kevin/Google Drive/datasci')
##-----Load Libraries-----
#install.packages("data.table")
library(dplyr)
library(data.table)
source('weather_retrieval.R')
# Load jittered Data
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
headcount$DateFormat = as.Date(headcount$DateFormat, format="%m/%d/%Y")
weather_file_name = 'las_vegas_hourly_weather.csv'
# Let's test if the file is in the directry using list.files()
#        If it is, load that file instead of running webscraper
if (weather_file_name %in% list.files()){
weather_data = read.csv(weather_file_name, stringsAsFactors = FALSE)
names(weather_data) = c('time','temp','dew_pt','humidity','pressure',
'visibility','wind_dir','wind_speed','gust_speed',
'precipitation','events','conditions',
'wind_dir_deg','date')
} else {
range(headcount$DateFormat)
airport = 'KLAS'
dates = seq(from=min(headcount$DateFormat),
to=max(headcount$DateFormat),
by=1)
weather_data = get_weather_data(airport, dates)
names(weather_data) = c('time','temp','dew_pt','humidity','pressure',
'visibility','wind_dir','wind_speed','gust_speed',
'precipitation','events','conditions',
'wind_dir_deg','date')
}
# Let's create a datetime in the weather data
weather_data$datetime = paste(weather_data$date,weather_data$time)
weather_data$datetime = strptime(weather_data$datetime, format="%Y-%m-%d %I:%M %p")
weather_data$Hour = as.numeric(format(round(weather_data$datetime, units="hours"), format="%H"))
# Let's merge with different methods.
#   - If we were truly merging to analyze casino data, we don't want to lose
#      headcount data if weather doesn't exist, so we want to do a
#      left merge (keeping all the headcount data)
#
#   - Remember, we want to merge on date AND hour.
#   - Note: the headcount data has multiple rows for these (more than 1 game type)
# Check for duplicates!
anyDuplicated(headcount[c("DateFormat", "Hour","GameCode")])
anyDuplicated(weather_data[c("date", 'Hour')]) # Oh no!  How could this happen?
# Drop for now:
weather_data = weather_data[!duplicated(weather_data[c("date", 'Hour')]),]
# Rename some columns:
intersect(names(headcount), names(weather_data))
weather_data$DateFormat = weather_data$date
weather_data$date = NULL
weather_data$DateFormat = as.Date(weather_data$DateFormat, format="%Y-%m-%d")
# Pick one of the below merges, and comment out the other two.
# <<<CHANGE BELOW MERGING CODE>>>
# Merge (base)
headcount_base_all = merge(headcount, weather_data, all.x=TRUE, by=c("DateFormat","Hour"))
# Merge(data.table)
# Note that data.table has a problem.  It canNOT take POSIX values. So we drop it (we are done with that column anyways)
weather_data$datetime = NULL
library(data.table)
headcount = as.data.table(headcount)
weather_data = as.data.table(weather_data)
# Set keys for faster merges
setkeyv(headcount, c("DateFormat", "Hour"))
setkeyv(weather_data, c("DateFormat", "Hour"))
headcount_dt_all = merge(headcount, weather_data, all.x=TRUE, by=c("DateFormat", "Hour"))
# Merge(dplyr)
library(dplyr)
headcount_dplyr_all = left_join(headcount, weather_data, by=c("DateFormat", "Hour"))
##----Find another insight involving weather------
# For now, drop all NA rows:
#     use the command 'complete.cases':
# Use 'complete.cases()' as a row filter on your data frame.
headcount_base_all$datetime = NULL
headcount_base_all = headcount_base_all[complete.cases(headcount_base_all),]
# can also use na.omit()
headcount_dplyr_all = na.omit(headcount_dplyr_all)
headcount_dt_all = na.omit(headcount_dt_all)
#observation 1 - Temperature and humidity
library(ggplot2)
avg_hum = aggregate(HeadCount ~ humidity, data = headcount_base_all, FUN = mean)
avg_temp = aggregate(HeadCount ~ temp, data = headcount_base_all, FUN = mean)
qplot(avg_hum$humidity,avg_hum$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Humidity %", ylab = "Headcount")
qplot(avg_temp$temp,avg_temp$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Temperature (F)", ylab = "Headcount")
#observation 2 - headcount by closing price gain/loss S&P500 index (from Yahoo finance csv export)
avg_by_day = aggregate(HeadCount ~ DayOfWeek, data = headcount_base_all, FUN = mean)
stock = read.csv('spclose.csv', stringsAsFactors = FALSE)
stock$DateFormat = as.Date(stock$DateFormat, format="%m/%d/%Y")
#select days 1-5 (since market is only open then)
weekDayHC <- subset(headcount_base_all, DayOfWeek <= 5)
avg_by_day = aggregate(HeadCount ~ DateFormat, data = weekDayHC, FUN = mean)
stockHCtest = merge(stock, weekDayHC, by = "DateFormat")
diffHC = merge(stock, avg_by_day, by = "DateFormat")
#nothing too interesting
qplot(diffHC$Diff,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Index gain/loss", ylab = "Headcount")
#over time
qplot(diffHC$DateFormat,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Date", ylab = "Headcount")
qplot(diffHC$Close,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Date", ylab = "Headcount")
diffHC = merge(stock, avg_by_day, by = "DateFormat")
#observation 3 compare headcount to google searches for las vegas (via google trends csv export). Unfortunately weekly
google = read.csv('google.csv', stringsAsFactors = FALSE,strip.white=TRUE)
#google$rawDate <- strsplit(google$Week, "-")
google$year <- sapply(strsplit(google$Week, "-"), '[',1)
google$month <- sapply(strsplit(google$Week, "-"), '[',2)
google$day <- trimws(sapply(strsplit(google$Week, "-"), '[',3))
google$DateFormat <- paste(google$month, google$day, google$year, sep = "/")
google$DateFormat = as.Date(google$DateFormat, format="%m/%d/%Y")
require(zoo)
googleHC <- merge(headcount_base_all, google, by = "DateFormat", all = TRUE)
googleHC<- na.locf(googleHC, na.rm = TRUE)
avg_by_day = aggregate(HeadCount ~ DateFormat, data = headcount_base_all, FUN = mean)
searchHC <- merge(google, avg_by_day,by = "DateFormat", all = TRUE)
searchHC <- na.locf(searchHC, na.rm = TRUE)
plot(searchHC$Index,searchHC$HeadCount)
?barplot
plot(searchHC$Index,searchHC$HeadCount)
?ggplot
ggplot(data = searchHC)
ggplot(searchHC, aes(Index, HeadCount)) + geom_point()
View(searchHC)
View(searchHC)
class(searchHC$HeadCount)
class(searchHC$Index)
searchHC$HeadCount <- as.numeric(as.character(searchHC$HeadCount))
searchHC$HeadCount <- as.numeric(as.character(searchHC$HeadCount))
searchHC$Index <- as.numeric(as.character(searchHC$Index))
ggplot(searchHC, aes(Index, HeadCount)) + geom_point()
plot(searchHC$Index,searchHC$HeadCount)
ggplot(searchHC, aes(Index, HeadCount)) + geom_point()
ggplot(searchHC, aes(Index, HeadCount)) + geom_line()
ggplot(searchHC, aes(Index, HeadCount)) + geom_point() + geom_smooth()
ggplot(searchHC, aes(Index, HeadCount)) + geom_point() + geom_smooth(method=lm)
ggplot(diffHC, aes(Close, HeadCount)) + geom_point() + geom_smooth(method=lm)
qplot(diffHC$Close,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Date", ylab = "Headcount")
ggplot(diffHC, aes(Close, HeadCount)) + geom_point() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount)) + geom_point() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount)) + geom_jitter()
ggplot(searchHC, aes(Index, HeadCount)) + geom_jitter() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount)) + geom_point() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount)) + geom_jitter() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount),
size=2, position = position_jitter(x = 2,y = 2) ) +
geom_jitter(colour=alpha("black",0.15) )
ggplot(searchHC, aes(Index, HeadCount),
size=2, position = position_jitter(x = 2,y = 2) ) +
geom_jitter(colour=alpha("black",0.15) )
diffHC
View(diffHC)
View(diffHC)
library(ggplot2)
avg_hum = aggregate(HeadCount ~ humidity, data = headcount_base_all, FUN = mean)
avg_temp = aggregate(HeadCount ~ temp, data = headcount_base_all, FUN = mean)
qplot(avg_hum$humidity,avg_hum$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Humidity %", ylab = "Headcount")
qplot(avg_temp$temp,avg_temp$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Temperature (F)", ylab = "Headcount")
avg_by_day = aggregate(HeadCount ~ DayOfWeek, data = headcount_base_all, FUN = mean)
stock = read.csv('spclose.csv', stringsAsFactors = FALSE)
stock$DateFormat = as.Date(stock$DateFormat, format="%m/%d/%Y")
#select days 1-5 (since market is only open then)
weekDayHC <- subset(headcount_base_all, DayOfWeek <= 5)
avg_by_day = aggregate(HeadCount ~ DateFormat, data = weekDayHC, FUN = mean)
stockHCtest = merge(stock, weekDayHC, by = "DateFormat")
diffHC = merge(stock, avg_by_day, by = "DateFormat")
#nothing too interesting
qplot(diffHC$Diff,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Index gain/loss", ylab = "Headcount")
#over time
qplot(diffHC$DateFormat,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Date", ylab = "Headcount")
qplot(diffHC$Close,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Date", ylab = "Headcount")
diffHC = merge(stock, avg_by_day, by = "DateFormat")
qplot(diffHC$Close,diffHC$HeadCount, geom=c("point", "smooth"),
method="lm", formula=y~x, xlab = "Closing price", ylab = "Headcount")
diffHC = merge(stock, avg_by_day, by = "DateFormat")
google = read.csv('google.csv', stringsAsFactors = FALSE,strip.white=TRUE)
#google$rawDate <- strsplit(google$Week, "-")
google$year <- sapply(strsplit(google$Week, "-"), '[',1)
google$month <- sapply(strsplit(google$Week, "-"), '[',2)
google$day <- trimws(sapply(strsplit(google$Week, "-"), '[',3))
google$DateFormat <- paste(google$month, google$day, google$year, sep = "/")
google$DateFormat = as.Date(google$DateFormat, format="%m/%d/%Y")
require(zoo)
googleHC <- merge(headcount_base_all, google, by = "DateFormat", all = TRUE)
googleHC<- na.locf(googleHC, na.rm = TRUE)
avg_by_day = aggregate(HeadCount ~ DateFormat, data = headcount_base_all, FUN = mean)
searchHC <- merge(google, avg_by_day,by = "DateFormat", all = TRUE)
searchHC <- na.locf(searchHC, na.rm = TRUE)
searchHC$HeadCount <- as.numeric(as.character(searchHC$HeadCount))
searchHC$Index <- as.numeric(as.character(searchHC$Index))
plot(searchHC$Index,searchHC$HeadCount)
ggplot(searchHC, aes(Index, HeadCount)) + geom_jitter() + geom_smooth(method=lm)
ggplot(searchHC, aes(Index, HeadCount),
size=2, position = position_jitter(x = 2,y = 2) ) +
geom_jitter(colour=alpha("black",0.15) )
ggplot(searchHC, aes(Index, HeadCount),
size=2, position = position_jitter(x = 2,y = 2) ) +
geom_jitter(colour=alpha("black",0.15) )
?alpha
require(ggplot2)
